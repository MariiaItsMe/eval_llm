# eval_llm
Large Language Models (LLMs) are known to generate factually incorrect answers. However, they do so with such fluency that it often requires extra attention and criticism to spot the mistakes. This work challenges that dependency by proposing and rigorously comparing alternative implementations for this metric, paving the way for more robust, transparent, and unbiased evaluation frameworks.



We utilize a custom, expert-generated dataset, that consists of 20 carefully formulated questions, each paired with a corresponding ground truth answer. In this project, we employ RAGAS prompts combined with multiple state-of-the-art LLMs: GPT-4o, Llama3.3, and NLI-DeBERTa-v3 to address both sentence splitting and classification tasks

## **Scripts and Utilities**
- **`app.py`** – Script for separating usage statements and performing evaluation using the Ragas framework.
- **`modifier.py`** – Used for modifying the outputs of statement extraction from `hacking_ragas.py` and `hacking_ragas_llama.py`.
- **`nli.py`** – Natural Language Interface script with statements generated by PyCharm and evaluated using NLI-DeBERTa-v3.
- **`nli_ragas.py`** – Statements are generated by the Ragas framework and evaluated using NLI-DeBERTa-v3.
- **`hacking_ragas_llama.py`** – Utility script used for extracting statements and modifying the LLM model used.
- **`statements_ragas.py`** – Utility script specifically for handling statement extraction.


## **Files**
- **`ragas_20_questions_dataset.csv`** – Dataset used for evaluation.
- **`cleaned_output.log`, `terminal_output.log`** – Various log files used for Ragas statement extraction.
- **`evaluation_results.csv`** – CSV file containing evaluation results.

## Setup
1. Clone the repository:
```
git clone <url>
cd <folder>
```
2. Install dependencies:
```
pip install -r requirements.txt
```
3. Run scripts for evaluation tasks using different approaches as needed.
