# eval_llm
This project is designed to evaluate a Language Model (LLM) on a custom dataset using various scripts and utilities.

## **Scripts and Utilities**
- **`app.py`** – Script for separating usage statements and performing evaluation using the Ragas framework.
- **`modifier.py`** – Used for modifying the outputs of statement extraction from `hacking_ragas.py` and `hacking_ragas_llama.py`.
- **`nli.py`** – Natural Language Interface script with statements generated by PyCharm and evaluated using NLI-DeBERTa-v3.
- **`nli_ragas.py`** – Statements are generated by the Ragas framework and evaluated using NLI-DeBERTa-v3.
- **`hacking_ragas_llama.py`** – Utility script used for extracting statements and modifying the LLM model used.
- **`statements_ragas.py`** – Utility script specifically for handling statement extraction.


## **Files**
- **`ragas_20_questions_dataset.csv`** – Dataset used for evaluation.
- **`cleaned_output.log`, `terminal_output.log`** – Various log files used for Ragas statement extraction.
- **`evaluation_results.csv`** – CSV file containing evaluation results.

## Setup
1. Clone the repository:
```
git clone <url>
cd <folder>
```
2. Install dependencies:
```
pip install -r requirements.txt
```
3. Run scripts for evaluation tasks using different approaches as needed.
